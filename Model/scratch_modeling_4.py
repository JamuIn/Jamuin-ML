# -*- coding: utf-8 -*-
"""scratch_modeling_4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/alhusari2/Capstone-ML/blob/experiment/scratch_modeling_4.ipynb

##import package
"""

import pandas as pd
import numpy as np
import itertools
import keras
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img
from keras.models import Sequential
from keras import optimizers
from keras.preprocessing import image
from keras.layers import Dropout, Flatten, Dense
from keras import applications
from keras.utils.np_utils import to_categorical
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import math
import datetime
import time
from keras.layers import LeakyReLU
from keras.callbacks import EarlyStopping
from keras.callbacks import ModelCheckpoint
from keras.models import load_model
import os
import tensorflow as tf

# print('Tensorflow_VER= V',tf.version.VERSION)
# print(confusion_matrix)

"""##Import dataset"""

# Mount Google Drive
#meninisialisasi drive
from google.colab import drive
drive.mount('/content/drive')

#set directory
train_data_dir = '/content/drive/MyDrive/machine learning/[1000]dataset VGG19 coba lagi/training'
validation_data_dir = '/content/drive/MyDrive/machine learning/[1000]dataset VGG19 coba lagi/validation'
test_data_dir = '/content/drive/MyDrive/machine learning/[1000]dataset VGG19 coba lagi/test'

"""##training and validation generator"""

#menginisialisasi variabel
batch_size = 8

def train_val_generators(TRAINING_DIR, VALIDATION_DIR,TEST_DIR):
  # Instantiate the ImageDataGenerator class
  # Don't forget to normalize pixel values and set arguments to augment the images
  train_datagen = ImageDataGenerator(rescale = 1./255.,
                                  zoom_range=0.2,
                                  horizontal_flip=True,
                                  vertical_flip=True)

  # Pass in the appropriate arguments to the flow_from_directory method
  train_generator = train_datagen.flow_from_directory(directory=TRAINING_DIR,
                                                      batch_size=batch_size,
                                                      class_mode='categorical',
                                                      target_size=(300, 300))

  # Instantiate the ImageDataGenerator class (don't forget to set the rescale argument)
  # Remember that validation data should not be augmented
  validation_datagen = ImageDataGenerator(rescale = 1./255.)

  # Pass in the appropriate arguments to the flow_from_directory method
  validation_generator = validation_datagen.flow_from_directory(directory=VALIDATION_DIR,
                                                                batch_size=batch_size,
                                                                target_size=(300, 300),
                                                                class_mode='categorical',
                                                                shuffle=False)

  # Instantiate the ImageDataGenerator class
  test_datagen = ImageDataGenerator(rescale = 1./255.)
  # Pass in the appropriate arguments to the flow_from_directory method
  test_generator = test_datagen.flow_from_directory(
                    TEST_DIR,
                    target_size=(300, 300),
                    batch_size=batch_size,
                    class_mode='categorical',
                    shuffle=False)

  return train_generator, validation_generator, test_generator

# Test the generators
train_generator, validation_generator, test_generator = train_val_generators(train_data_dir, validation_data_dir, test_data_dir)

num_classes = len(train_generator.class_indices)
print(num_classes)

"""##model"""

model = tf.keras.models.Sequential([
    # Note the input shape is the desired size of the image 300x300 with 3 bytes color
    # This is the first convolution
    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),
    tf.keras.layers.MaxPooling2D(2, 2),
    # The second convolution
    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    # The third convolution
    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    # The fourth convolution
    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    # The fifth convolution
    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    # Flatten the results to feed into a DNN
    tf.keras.layers.Flatten(),
    # 512 neuron hidden layer
    tf.keras.layers.Dense(512, activation='relu'),
    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans')
    tf.keras.layers.Dense(3, activation='softmax')
])

model.summary()

# Set the training parameters
model.compile(optimizer = tf.keras.optimizers.Adam(),
              loss = 'categorical_crossentropy',
              metrics = ['accuracy'])

class myCallback(tf.keras.callbacks.Callback):
        # Define the correct function signature for on_epoch_end
        def on_epoch_end(self, epoch, logs={}):
            if(logs.get('accuracy') > 0.95):
                print("\nReached 95% accuracy so cancelling training!")
                # Stop training once the above condition is met
                self.model.stop_training = True

"""##evaluate model"""

# Train the model.
callbacks = myCallback()
history = model.fit(train_generator,
      epochs=20,
      batch_size=batch_size,
      validation_data=validation_generator,
      verbose=1,
      callbacks=[callbacks])

import matplotlib.pyplot as plt

# Plot the results
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

epochs = range(len(acc))

plt.plot(epochs, acc, 'r', label='Training accuracy')
plt.plot(epochs, val_acc, 'b', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.legend(loc=0)
plt.figure()

plt.show()

import matplotlib.pyplot as plt

# Plot the results
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, loss, 'r', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend(loc=0)
plt.figure()

plt.show()

"""##evaluasi"""

test_labels = test_generator.classes
test_labels = to_categorical(test_labels, num_classes=num_classes)

preds = np.round(model.predict(test_generator),0)
print('rounded test_labels', preds)

# Mendapatkan daftar kelas berdasarkan nama subfolder di folder tersebut
view = os.listdir(test_data_dir)
print(view)

classification_metrics = metrics.classification_report(test_labels, preds, target_names=view )
print(classification_metrics)

import numpy as np
from sklearn.metrics import confusion_matrix

# Mendapatkan prediksi dari model
y_pred = model.predict(test_generator)
y_pred = np.argmax(y_pred, axis=1)  # Mengambil indeks dengan nilai terbesar sebagai label prediksi

# Mendapatkan label sebenarnya
y_true = test_generator.classes

# Mencetak confusion matrix
cm = confusion_matrix(y_true, y_pred)
print("Confusion Matrix:")
print(cm)

from sklearn.metrics import confusion_matrix

categorical_test_labels = pd.DataFrame(test_labels).idxmax(axis=1)
categorical_preds = pd.DataFrame(preds).idxmax(axis=1)
confusion_matrix= confusion_matrix(categorical_test_labels, categorical_preds)

plot_confusion_matrix(confusion_matrix, view)

"""##saving model"""

# model.save_weights('scratch_modeling_weight_4.h5')
model.save_weights('/content/drive/MyDrive/machine learning/scratch_modeling_weight_4.h5')
model.save('/content/drive/MyDrive/machine learning/model_scracth_jamuin.h5')

"""##testing"""

def read_image(file_path):
    print("[INFO] loading and preprocessing image...")
    image = load_img(file_path, target_size=(300, 300))
    image = img_to_array(image)
    image = np.expand_dims(image, axis=0)
    image /= 255.
    return image

def upload():
  from google.colab import files
  uploaded = files.upload()
  for name, data in uploaded.items():
    with open(name, 'wb') as f:
      f.write(data)
      print('Saved file:', name)
      return name


def test_single_image(path):
    images = read_image(path)
    view = ['jahe', 'kunyit', 'lengkuas']
    time.sleep(.5)
    preds = model.predict(images)

    max_index = np.argmax(preds[0])  # Mendapatkan indeks label dengan probabilitas tertinggi
    max_label = view[max_index]  # Mendapatkan label dengan probabilitas tertinggi

    print("Hasil Tebakan: {}".format(max_label))
    print("Probabilitas: {}%".format(round(preds[0][max_index] * 100, 2)))
    print()

    return load_img(path)

model = load_model('/content/drive/MyDrive/machine learning/model_scracth_jamuin.h5')

name = upload()
path = str(name)
test_single_image(path)



